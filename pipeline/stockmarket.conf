input {
  file {
    path => "/usr/share/logstash/kaggle_dataset/indexProcessed.csv"
    start_position => "beginning"
    sincedb_path => "/dev/null"
  }
}

filter {
  csv {
    separator => ","
    columns => [ "Index", "Date", "Open", "High", "Low", "Close", "AdjClose", "Volume", "CloseUSD"]
  }
  mutate {
    remove_field => ["host", "path", "message", "@version"] #@timestamp is a protected field
    convert => {
      "Open" => "integer"
      "High" => "integer"
      "Low" => "integer"
      "Close" => "integer"
      "AdjClose" => "integer"
      "Volume" => "integer"
      "CloseUSD" => "integer"
    }
  }
}

output {
    mongodb {
      id => "monolitico"
      uri => "mongodb://172.30.10.175:27017/proyecto"  #vpn y puerto del mongo server de luisdi
      database => "proyecto"
      collection => "stocks"
      codec => "json"
    }

    mongodb {
      id => "cluster"
      uri => "mongodb://localhost:27023/proyecto"  #vpn y puerto del router de pamela
      database => "proyecto"
      collection => "stocks"
      codec => "json"
    }

    jdbc {
		connection_string => 'jdbc:postgresql://localhost:5432/proyecto?user=admin&password=12345'  # localhost de daniel
		statement => [ "INSERT INTO stocks (High, Close, Low, Date, Index, Timestamp, Open, AdjClose, CloseUSD, Volume) VALUES(?, ?, ?, ?, ? ,CAST (? AS Timestamp), ?, ?, ?, ?)", "High", "Close", "Low", "Date", "Index", "@timestamp", "Open", "AdjClose", "CloseUSD", "Volume"]
	  }

}


