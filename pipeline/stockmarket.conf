input {
  file {
    path => "/usr/share/logstash/kaggle_dataset/indexProcessed.csv"
    start_position => "beginning"
    sincedb_path => "/dev/null"
  }
}

filter {
  csv {
    separator => ","
    columns => [ "Index", "Date", "Open", "High", "Low", "Close", "AdjClose", "Volume", "CloseUSD"]
    skip_header => true
  }
  mutate {
    remove_field => ["host", "path", "message", "@version"] #@timestamp is a protected field
    convert => {
      "Open" => "integer"
      "High" => "integer"
      "Low" => "integer"
      "Close" => "integer"
      "AdjClose" => "integer"
      "Volume" => "integer"
      "CloseUSD" => "integer"
    }
  }
}

output {
    mongodb {
      id => "monolitico"
      uri => "mongodb://172.30.10.175:27017/stockdb"  #vpn y puerto del mongo server de Pame
      database => "stockdb"
      collection => "daily_prices"
      codec => "json"
    }

    mongodb {
      id => "cluster"
      uri => "mongodb://localhost:27023/stockdb"  #puerto del router de Daniel (el que corre el logstash)
      database => "stockdb"
      collection => "daily_prices"
      codec => "json"
    }

    jdbc {
		connection_string => 'jdbc:postgresql://localhost:5432/stockdb?user=admin&password=12345'  # localhost de daniel
		statement => [ "INSERT INTO daily_prices (High, Close, Low, Date, Index, Timestamp, Open, AdjClose, CloseUSD, Volume) VALUES(?, ?, ?, ?, ? ,CAST (? AS Timestamp), ?, ?, ?, ?)", "High", "Close", "Low", "Date", "Index", "@timestamp", "Open", "AdjClose", "CloseUSD", "Volume"]
	  }

}


